FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 8 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (8, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -4.60410870012952173624e+01) (1, 1.63370356872719035835e+01) (2, 9.91020210276087176737e+00) (3, 3.09069265232507355279e+01) (4, -1.43719183742274392301e+01) (5, 5.38281136774702240189e-01) (6, 2.60439104865184027915e+01) (7, -1.03493386800539042270e+01) (0, 1.07985840885539285949e+01) (1, -6.70006920203468947506e+00) (2, -6.15694437756897139380e-01) (3, -5.55221299378400701130e-01) (4, 2.39963964228956294633e+00) (5, -2.29272082979619362675e+00) (6, -9.76610536306808540985e+00) (7, -2.31152903376711060801e+00) (0, -1.24327683200142155329e+03) (1, 8.46757426756756672148e+02) (2, -5.28579397758525942663e+02) (3, 2.60138515566480862162e+02) (4, -1.33693233334098131593e+01) (5, -8.01275268207731272696e+01) (6, 3.19752469423005948101e+02) (7, -2.34647984167018774926e+00) (0, -1.81576204770680824652e+01) (1, 6.71399967982740442096e+00) (2, 2.10330986156543131571e+00) (3, 1.48151747838742711139e+01) (4, -5.65541458893391357776e+00) (5, 1.29593616451805315748e+00) (6, 1.14515029168205373367e+01) (7, -6.06159781772189454330e+00) (0, 3.66702247083333290334e+01) (1, -1.32739079357483902299e+01) (2, -2.35269835144827199613e+00) (3, 2.47038464292135913070e+01) (4, -3.33298458412564926334e+01) (5, -3.93860329904053187988e+01) (6, -1.85330581129146132469e+01) (7, 7.48348511719575526513e+00) (0, 4.51948904717849984536e+02) (1, 1.71107572438536351456e+02) (2, -1.08115259785271791770e+03) (3, 1.03034956836988317264e+03) (4, -1.10369109644505829237e+03) (5, -6.34963029592756129205e+02) (6, 4.34734775691038237255e+02) (7, -1.19764133878177361225e+02) (0, 1.24286646067342462629e+03) (1, -6.65946718487285806987e+02) (2, -5.43752558817296360871e+01) (3, 2.21578450628395444255e+02) (4, -1.87355603233848114542e+02) (5, 1.47285981485495852894e+02) (6, -1.50000000000000000000e+03) (7, -4.56838032018150812519e+01) (8, 3.64856404368241227232e+00) (9, -1.37964029284165548006e+01) (10, -3.23836283990963957891e+00) (11, -4.52186237891287845514e+00) (12, 2.39239577096478450713e+00) (13, -2.69422991184860682878e+00) (14, 1.99242944427614143699e+00) (15, 5.30032969920687602183e-01) 
