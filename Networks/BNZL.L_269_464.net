FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 8 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (8, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -4.09252614230088695990e+01) (1, -9.92531007215591110082e+00) (2, -7.20263319683166614027e+00) (3, -1.14909105936898114919e+01) (4, -7.49900035788876984100e+00) (5, 6.06104823115276403200e+01) (6, 2.55006020469084546676e+01) (7, 3.41770023204894934921e+00) (0, 5.39193183399066668926e+01) (1, -5.35716621336019613864e+01) (2, 2.45990810964362154323e+01) (3, -2.01152184448523485116e+02) (4, 6.07422544509802833090e+01) (5, -5.43122759831048185220e+01) (6, -4.24039406194819505913e+01) (7, 7.97313765162558070188e+01) (0, 1.01077943195510363239e+03) (1, -6.95424812486660215427e+01) (2, 9.49016570658718023878e+02) (3, -1.50000000000000000000e+03) (4, 2.01589686546599779149e+02) (5, 5.27868555039044736077e+02) (6, -1.25746290643595966685e+03) (7, -1.29519816017548265563e+02) (0, -2.85333535648391078965e+00) (1, 1.11170882164437330175e+01) (2, 9.94840346389996810217e+00) (3, -2.59965004356892563919e+01) (4, 7.18200123950381197346e+00) (5, 4.26396805096866504670e+00) (6, -8.20691657213278347172e+00) (7, -1.05145341502421385549e-01) (0, 2.72757614732883894249e+02) (1, 1.49939396114127112014e+03) (2, -1.02457375251624057455e+03) (3, -9.89760876261367599227e+01) (4, -1.50000000000000000000e+03) (5, -1.50000000000000000000e+03) (6, -8.31210455102485752832e+02) (7, -1.39607491455231979671e+02) (0, 4.55980929167669728486e+02) (1, 1.91780633374853238138e+02) (2, 9.04046164238040205419e+02) (3, -1.50000000000000000000e+03) (4, 2.36245150634696585712e+02) (5, 7.56099858378422823080e+02) (6, -8.71592159143931212384e+02) (7, 2.09504298732173879216e+01) (0, -3.08285214259566338058e+01) (1, 6.40021005564447818870e+02) (2, -1.14377700239706018692e+03) (3, -1.49599710872512264359e+03) (4, -1.50000000000000000000e+03) (5, -1.50000000000000000000e+03) (6, 2.80917956065466228210e+02) (7, 5.90154329202936651200e+02) (8, -1.83218853039786955250e+00) (9, 1.71818911464081969243e+00) (10, 2.95118217703548779696e+00) (11, -4.92896493359774012788e+00) (12, -1.08866032204448237586e+00) (13, 2.15372030777117107192e+00) (14, -1.00136061086583105784e+00) (15, 2.39928104335976194461e-01) 
