FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 8 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (8, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.14856325861583925985e+02) (1, 1.78066850167132287197e+02) (2, 1.52995971945574893880e+02) (3, 2.09062628894205282393e+01) (4, -8.75832585206734819394e+01) (5, -3.96563768908898239829e+01) (6, -1.20136413732205397764e+02) (7, -1.22696437914570566363e+02) (0, 3.77889511390406624969e+01) (1, 1.35227665619656942653e+01) (2, 1.26954745983017559752e+02) (3, 1.35651809548183188703e+00) (4, -7.83295321401944448780e+01) (5, 4.99068939859207816312e+00) (6, -1.95520159333804910418e+00) (7, -5.31677450039265764303e+01) (0, -2.26574299289073133323e+00) (1, -1.88112687190842536866e-01) (2, -6.84196892280002444409e+00) (3, 6.66376391142936785883e+00) (4, 5.31757983777014331395e-01) (5, 1.29498085992743039441e+00) (6, -1.55159947418424093435e+00) (7, 1.19293032363313367128e+00) (0, -4.47605188279123012762e+02) (1, 1.62303593939912227384e+00) (2, 4.57048869289707013763e+01) (3, 5.12708199155982697448e+02) (4, -7.22113086062812953969e+02) (5, 4.83144488496249323362e+02) (6, -1.98429165622028705229e+02) (7, 1.95077143881007344817e+02) (0, 1.85007177306916219095e+02) (1, 1.17673631441104362239e+02) (2, -1.49994375788231627666e+03) (3, 8.32020357943627345776e+02) (4, -1.50000000000000000000e+03) (5, 3.66489877067706856906e+02) (6, 3.90793137124932741244e+02) (7, 9.63175400521721627456e+01) (0, -1.35352512515964598094e+01) (1, 2.69334319406624977944e+01) (2, 3.72032726708574870145e+01) (3, -4.71948719831558420879e+01) (4, -1.18278160083068275554e+01) (5, 1.02643571581791466230e+01) (6, -2.40453515133475150378e+02) (7, 2.92294595733797857307e+00) (0, 4.74239246902167792541e+01) (1, -3.16369608608045389531e+01) (2, 1.57231980836985330008e+00) (3, 5.30342546414118416465e+01) (4, 6.83394253534003297545e+01) (5, -1.42677618055672610353e+01) (6, -5.92493873345907786643e+01) (7, -2.91298345172402584069e+00) (8, 4.31897401352527232632e+00) (9, -4.50430018530849896052e+00) (10, -2.30870865069227271249e+00) (11, 1.41329738159324547198e+00) (12, 8.59193011925968264109e+00) (13, 2.45601785835060537622e+00) (14, 8.08795408983408137260e+00) (15, -7.45348838885594933146e+00) 
