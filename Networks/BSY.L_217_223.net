FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 8 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (8, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 7.28512476433969102807e-01) (1, 1.03780093307512597534e+01) (2, 1.42051698605686400612e+01) (3, -9.07152073731335484297e+00) (4, -1.82904392380301494825e+01) (5, -2.87199398591158399086e+01) (6, -7.01188762454520198020e+00) (7, -1.54158639932028016517e+01) (0, 3.56690020944852506091e+02) (1, 1.38186286677157710301e+02) (2, 1.16156998363270781738e+03) (3, -3.31459090099650154571e+02) (4, -5.52194170447565625182e+02) (5, -2.91387790347551629111e+02) (6, -9.77377706321272626155e+02) (7, -1.21481469724302939994e+02) (0, 6.32597096575927366757e+02) (1, 7.33343250219172091420e+02) (2, -8.83118481657259167150e+02) (3, -2.68415877643585702117e+02) (4, 1.24783998193543311572e+02) (5, 1.08180425762643835697e+02) (6, -1.50000000000000000000e+03) (7, 3.33559761933320316984e+02) (0, 2.10850567157410182517e+01) (1, 1.86702699135506300365e+01) (2, -2.58153023826815903874e+01) (3, -7.98723910978285971396e+01) (4, 6.71224558704022769007e+01) (5, 6.72666949511961860253e+01) (6, -5.33303124612707364349e+01) (7, -2.23469628769727250983e+01) (0, -8.18002979366402627193e+01) (1, 3.84796090354063906602e+01) (2, -6.60711967976373983902e+01) (3, -3.64008098981076457790e+01) (4, 1.09101530317769253031e+01) (5, 6.17222588048494174018e+01) (6, -1.18196954423968936254e+01) (7, -9.47598386132514569624e+00) (0, 4.13463028851810676656e+01) (1, 3.11143233333599994239e+02) (2, -1.45124243133995349808e+02) (3, -3.97202984344174439002e+01) (4, -6.42766426181122674244e-01) (5, -1.89047383117146772236e+01) (6, -3.09691438532323218169e+02) (7, 7.30343668320819716655e+01) (0, 1.33605961527507560049e+02) (1, -1.94226469904942177891e+02) (2, 1.51825718972967621312e+02) (3, -7.63127905311279022271e+01) (4, 1.25031482933817148506e+01) (5, -2.57385532797154681361e+01) (6, -4.30962169563033441477e+01) (7, 6.12773756239121780709e+00) (8, -1.42675270535415577022e+00) (9, -3.32887053060708315577e-01) (10, -1.74628894695156633432e+00) (11, 3.14096631616237020879e-01) (12, 9.40842508265289922953e-01) (13, 1.40938982669103829437e+00) (14, 1.26781668292552973654e+00) (15, -3.48194860055118349251e-01) 
