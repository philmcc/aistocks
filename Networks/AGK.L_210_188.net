FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 8 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (8, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.52056281383455815792e-01) (1, -9.07974994025979142975e+00) (2, -3.63199051509542947258e+00) (3, -2.79858752055496884381e+00) (4, 5.36326995182604093770e+00) (5, 8.65897822685596629810e+00) (6, -4.42472141243791572407e+00) (7, 1.81107055460178556494e+00) (0, 2.08451389999227600924e+01) (1, 3.23214311275785775024e+01) (2, 3.85210519077243640496e+00) (3, -1.98800398094061847587e+01) (4, 1.73037619640413211641e+01) (5, -1.26901578711181191039e+01) (6, -2.99979382636374936055e+01) (7, -1.60837870572620147414e+01) (0, 4.28933956219728926840e+01) (1, -2.03557414027503185139e+01) (2, 2.58895275083886993173e+01) (3, -3.12194631247701401833e+01) (4, -1.34481953967442819931e+01) (5, 2.62039696536036426089e-01) (6, -3.31329277361782033040e+01) (7, 5.09146458990388861565e+00) (0, 1.54950796337964050053e+02) (1, -1.48555316673778889935e+02) (2, 1.63630753678597471890e+02) (3, 6.13181300044264219196e+01) (4, -1.98373935386490729549e+00) (5, 3.70789522571291115582e+02) (6, -5.75910041875489014274e+02) (7, 4.98190563727543107575e+01) (0, -8.33332021581072446281e+00) (1, -3.73429210873624697342e+01) (2, 8.21824031219904993506e-01) (3, 1.57951707173960542718e+01) (4, -1.72583492154459605672e+01) (5, 1.25702184454210907916e+01) (6, 2.22334171464577927679e+01) (7, 1.49201758738025986872e+01) (0, -7.69739671421210118751e+02) (1, -9.86703046576274687141e+02) (2, 2.09895360684821469022e+02) (3, 5.87591156568469614285e+02) (4, -9.39963816184367146889e+02) (5, 5.98420230772131503727e+02) (6, -1.50000000000000000000e+03) (7, 2.23718286927514554918e+02) (0, -5.89093445946508495581e+02) (1, -1.50000000000000000000e+03) (2, -1.50000000000000000000e+03) (3, 4.25202240083366859835e+02) (4, 2.42684144305720565171e+02) (5, 1.23682223352487153534e+03) (6, 2.82631228681943525771e+02) (7, 4.31288298335976492126e+02) (8, -3.69763245029622300564e+00) (9, 7.43102778056593660239e+00) (10, -3.05588762577889472283e+00) (11, 1.61316429960913465536e+00) (12, 6.78079410818309646913e+00) (13, 2.09473463274843130222e+00) (14, 2.33824036356104958401e+00) (15, -6.77222583901030183995e+00) 
