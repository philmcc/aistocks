FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 9 3 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -8.56578061082632069567e-02) (1, -5.37348556888826919820e-02) (2, -9.35480385988658180718e-02) (3, -6.53968750930856712600e-02) (4, 7.28888615651258897632e-03) (5, 2.22584193163874594079e-02) (6, 7.71089487092040115623e-03) (7, -3.45082935431345258492e-02) (0, -1.75242271091673798566e-02) (1, -3.96109103539721632559e-02) (2, 1.47773917172460481084e-02) (3, 9.22952884591413669213e-02) (4, 9.01287858766136318334e-02) (5, 8.73177949312000517557e-03) (6, 1.71314402242219443462e-02) (7, 6.44373711487258998831e-02) (0, -6.03683271127689810576e-02) (1, -5.67665285507817224397e-03) (2, -1.26684352780645353986e-02) (3, -2.70814689186434226498e-02) (4, -3.26761533718727958941e-03) (5, 5.43870859501821568571e-02) (6, -9.82224312346045569377e-02) (7, 7.02403548395686416761e-02) (0, -9.48493749998416540992e-02) (1, 9.72981328673382384054e-02) (2, 1.73433984420209852395e-02) (3, -2.80721344511528914101e-02) (4, -5.80077905469803464800e-02) (5, 9.96525164641814675814e-02) (6, 2.35613846005705632192e-02) (7, -4.36655951651274340519e-02) (0, -5.40823407148173437853e-02) (1, 3.00133474918208575932e-02) (2, -9.06246876809699286603e-03) (3, 5.32065470249436234251e-02) (4, -4.77282346819078023836e-02) (5, 9.86484275929395276750e-02) (6, -8.13017480083070287478e-02) (7, 3.47475396990409371445e-02) (0, -4.09624842511487549657e-02) (1, 3.34756451990551456843e-02) (2, 2.70428266680661777421e-02) (3, -5.08336998646512494560e-02) (4, -5.77925767979409685249e-02) (5, -5.58257345046957384471e-02) (6, -8.63963301129092031694e-02) (7, -1.81609024205938301977e-02) (0, 3.84976141303422086937e-02) (1, 9.35236099142373877857e-04) (2, 5.47576301508788665373e-02) (3, -6.47700026969611902805e-02) (4, -4.46776794407915886498e-02) (5, 5.65352004063904220454e-02) (6, -9.45296493475086679892e-02) (7, -3.95270529505171233642e-02) (0, 5.38333317836125341271e-02) (1, 2.28137506777606954844e-02) (2, 3.24008140884461046105e-02) (3, 9.58255427267483139708e-02) (4, 2.24662656518260436811e-02) (5, -4.40378027079671927058e-02) (6, -4.78400539284952464048e-02) (7, 6.83839265202570711910e-02) (8, 8.59755462739697773333e-02) (9, 4.30974787935238801140e-02) (10, 2.15904720550845821703e-02) (11, -6.17526898049218786468e-02) (12, 4.17459049894795472535e-02) (13, 4.02887255368936797462e-02) (14, 7.29948513842351709435e-02) (15, -9.92165807517853270969e-02) (16, -2.62356307541672939543e-02) (8, 3.76766553174950891147e-05) (9, -5.00502790331881983188e-02) (10, 1.59717939380078569056e-02) (11, 4.42119436407378760268e-02) (12, -3.64466076559812890423e-02) (13, 9.78108931006624049420e-02) (14, -1.72904437190360346643e-02) (15, 6.44886299332771972814e-02) (16, 5.25685217614251520946e-02) 
