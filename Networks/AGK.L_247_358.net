FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 8 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (8, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -2.65312808360475912650e+02) (1, 2.55382743294977636594e+01) (2, 1.45043198872603852578e+02) (3, 1.07857481580690972578e+02) (4, -1.77570906368630744510e+02) (5, 4.98960665139841950122e+01) (6, 7.10330075441505925937e+01) (7, -2.86092069128152530766e+01) (0, 4.80255350556928195527e-01) (1, 2.04132919303618116658e+01) (2, -5.35898103589956704695e+01) (3, -5.89164549995233599589e+00) (4, 2.97448521506155678651e+01) (5, -5.08891489186293739522e+01) (6, -3.99017676591286063470e+00) (7, 1.02203924634110308034e+01) (0, 7.18728807774047254497e+02) (1, 1.09686610577038678116e+02) (2, -1.35742601071668286750e+03) (3, 2.17236401003429449474e+02) (4, 7.91480335844618139163e+02) (5, -1.50000000000000000000e+03) (6, -3.72447342946739695435e+02) (7, 1.36561424885975839061e+02) (0, -7.58083263354453009697e+01) (1, -2.77047750342015433489e+02) (2, 5.06919937170690332096e+02) (3, 4.24962991073436626266e+02) (4, -4.72101712461808915577e+02) (5, 3.96940335565581733590e+02) (6, -6.77915698115908185173e+01) (7, -2.75851775986882671532e+02) (0, 2.83588211156850718453e+02) (1, 6.31330142718534261803e+02) (2, -1.17980808523812379462e+03) (3, -5.33973428616812952896e+01) (4, 6.83632776001304364399e+02) (5, -4.62713087530894938482e+02) (6, 5.96416774408133676388e+00) (7, -5.59397523456239653683e+01) (0, -2.87180346278024636320e+01) (1, -2.02806876186620783642e+01) (2, -8.49035332039493795264e+01) (3, 1.69706523576585311730e+02) (4, -8.74879816484375396612e+01) (5, 7.84502096388223009171e+01) (6, 2.37490752957880104645e+01) (7, -3.25630145913604067687e+01) (0, -2.87222739475374680751e+02) (1, -1.18954947303199219277e+03) (2, 3.81326791176008896400e+02) (3, -1.27252621115684618758e+02) (4, 3.57502156818422278661e+01) (5, -8.10111040861061670881e-01) (6, 1.09095723299214910185e+03) (7, -1.81509140282708898440e+02) (8, -1.31651727440918020484e+00) (9, 2.54439075636518952095e+00) (10, -1.64523857592015065876e+00) (11, -1.62316449701052807619e-02) (12, -1.14234513072518417687e+00) (13, -9.31738706734658084230e-01) (14, -1.26614334367074943266e+00) (15, 7.73298190505181448628e-01) 
