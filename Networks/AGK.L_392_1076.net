FANN_FLO_2.1
num_layers=4
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 10 10 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (10, 4, 5.00000000000000000000e-01) (10, 4, 5.00000000000000000000e-01) (10, 4, 5.00000000000000000000e-01) (10, 4, 5.00000000000000000000e-01) (10, 4, 5.00000000000000000000e-01) (10, 4, 5.00000000000000000000e-01) (10, 4, 5.00000000000000000000e-01) (10, 4, 5.00000000000000000000e-01) (10, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (10, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 4.81460919785332293941e+02) (1, 1.13546736584143559412e+03) (2, -1.48728801366655147831e+03) (3, 1.49454366302916196219e+03) (4, -1.50000000000000000000e+03) (5, -1.49455282259930368127e+03) (6, 4.44994436160439533978e+02) (7, -1.22794711116933399353e+03) (0, 1.49991763416761818917e+03) (1, 1.50000000000000000000e+03) (2, -1.16311445142995376045e+03) (3, 1.50000000000000000000e+03) (4, -1.50000000000000000000e+03) (5, -1.42155499835660407371e+03) (6, -5.14895803315960620239e+02) (7, 1.47869590693918644320e+03) (0, -1.38262412583105651720e+03) (1, 9.83540267277331167861e+02) (2, -6.37348219315611231650e+02) (3, -1.19926259819583424360e+03) (4, 1.28606804002705439416e+03) (5, 1.56246397777582728850e+02) (6, 1.05107717790244896605e+03) (7, -9.72744949943530627934e+01) (0, -8.31493178543844123851e+02) (1, 7.61930358486419407882e+01) (2, 3.03050469804722624190e+01) (3, -8.71180892807741088291e+02) (4, 8.53355356525394540768e+02) (5, 1.00380271134586234894e+03) (6, -1.33410289312896429692e+03) (7, 1.47548514664312847344e+03) (0, 3.12340238290686556866e+02) (1, -5.72264473952364710385e+02) (2, -1.21744528440043063711e+03) (3, -1.02416788699699350218e+03) (4, 8.83726908257300124205e+02) (5, 9.21456126704509529191e+02) (6, 3.49385934409733124539e+02) (7, 2.44607767698037292803e+02) (0, -3.58800537737287186246e+01) (1, -1.58440721229932876213e+02) (2, 1.18393228153658412793e+02) (3, -4.89921851162927382006e+01) (4, 4.34369197768584598407e+01) (5, -5.48568584115155530867e+01) (6, 1.31185249287733910251e+02) (7, -1.05687196778498240235e+01) (0, -6.51355301495930461897e+02) (1, -1.12014875418184647060e+03) (2, 3.43783615199731514167e+02) (3, 1.35664245277310146776e+03) (4, 1.49866310294342542875e+03) (5, -1.05435488238664675009e+03) (6, 2.40792976875740066589e+02) (7, 9.96507887228947026870e+01) (0, 1.45867563029962887811e+01) (1, 3.59081149202289509503e+02) (2, -1.02196113951831523536e+03) (3, 1.50000000000000000000e+03) (4, 1.33584154580075278318e+03) (5, -4.98813827066255612408e+02) (6, -1.36649740006547335724e+03) (7, 4.42224359942549540392e+02) (0, -3.20354019425062460869e+02) (1, 8.08214651444400971059e+01) (2, -4.38285802531485160216e+02) (3, -1.20610536414145167328e+03) (4, 8.07474403502900031526e+02) (5, -4.06208373247244693971e+01) (6, -3.49670166395649005153e+02) (7, 1.17630915738933867942e+03) (8, -2.70861387362056916572e+02) (9, -6.31528446334704767651e+02) (10, -4.48437382855945145366e+02) (11, 8.81212944428516266271e+02) (12, 1.48491058126180405452e+02) (13, -3.36661378159099513141e+02) (14, 9.87966546788273035418e+02) (15, -1.43763999390447725091e+03) (16, -8.80643069371066303574e+02) (17, 6.22863448146917335180e+02) (8, 3.58935815680770929248e+02) (9, 3.48737703722505727910e+02) (10, 2.16586255082168975150e+02) (11, -8.88475514843639402329e+00) (12, 1.46250225149130301361e+02) (13, 3.75838498348033965613e+02) (14, -1.50981227919542618565e+02) (15, -1.34105763000013894271e+03) (16, 3.68313724107168923183e+02) (17, 4.11465828339290055737e+02) (8, -2.94707698400689992013e+02) (9, -6.68845801175135761696e+02) (10, -1.50000000000000000000e+03) (11, 1.48907341974984979061e+03) (12, -1.50000000000000000000e+03) (13, -1.50000000000000000000e+03) (14, 1.15846434821184152497e+03) (15, -1.38582671146585266797e+03) (16, -1.50000000000000000000e+03) (17, 1.19435644289279412078e+03) (8, -3.48930727653160275281e+01) (9, 1.49806799346476122992e+03) (10, -1.49238305659510797341e+03) (11, 1.50000000000000000000e+03) (12, -1.49849251988721334783e+03) (13, -1.44461553529960247033e+03) (14, 4.43487711013964371887e+01) (15, 3.94645987967856513023e+02) (16, -3.51553966133321523557e+02) (17, -4.24359112969474310262e+01) (8, -1.25774135210896224635e+03) (9, -1.50000000000000000000e+03) (10, -1.50000000000000000000e+03) (11, 9.56649918344305660867e+01) (12, -1.50000000000000000000e+03) (13, -1.50000000000000000000e+03) (14, -1.49705409032284251225e+02) (15, -1.50000000000000000000e+03) (16, -1.50000000000000000000e+03) (17, 1.35747227485436155803e+03) (8, -1.50000000000000000000e+03) (9, -1.50000000000000000000e+03) (10, -1.50000000000000000000e+03) (11, 1.50000000000000000000e+03) (12, -1.50000000000000000000e+03) (13, -1.50000000000000000000e+03) (14, 1.50000000000000000000e+03) (15, -1.50000000000000000000e+03) (16, -1.50000000000000000000e+03) (17, 4.76482507527109419243e+02) (8, 1.50000000000000000000e+03) (9, 1.50000000000000000000e+03) (10, 1.50000000000000000000e+03) (11, -1.50000000000000000000e+03) (12, 1.50000000000000000000e+03) (13, 1.50000000000000000000e+03) (14, -1.47500000000000000000e+03) (15, 1.50000000000000000000e+03) (16, 1.50000000000000000000e+03) (17, -3.62809424681301834426e+02) (8, 1.50000000000000000000e+03) (9, 2.80477669537638462316e+02) (10, 1.50000000000000000000e+03) (11, -1.50000000000000000000e+03) (12, 1.50000000000000000000e+03) (13, 1.50000000000000000000e+03) (14, -1.50000000000000000000e+03) (15, 7.96314181924792706013e+02) (16, 1.48637525154837362606e+03) (17, 3.37394820539668955917e+02) (8, -1.50000000000000000000e+03) (9, -1.50000000000000000000e+03) (10, -1.50000000000000000000e+03) (11, 1.50000000000000000000e+03) (12, -1.50000000000000000000e+03) (13, -1.50000000000000000000e+03) (14, 1.50000000000000000000e+03) (15, -1.50000000000000000000e+03) (16, -1.50000000000000000000e+03) (17, -6.64652518167097014157e+02) (18, -9.85139294097537171480e-02) (19, 2.41179885302791940926e-01) (20, 4.63726452544988496096e-02) (21, 2.04037444329862066583e-01) (22, 1.45368421654517536723e+01) (23, 6.97954494655386170621e-01) (24, -1.21189691328930426728e+00) (25, -5.51801127445583849962e-01) (26, 1.50000000000000000000e+03) (27, 1.50904291276815438749e+00) 
