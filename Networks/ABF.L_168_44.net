FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 6 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (6, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 7.36238954480979117534e+01) (1, -1.30822831020985915984e+02) (2, -4.63666348820252451901e+01) (3, 1.12589313500110833388e+02) (4, 2.12914785600396569976e+01) (5, 9.72955457052575312105e+00) (6, -4.19931253883591821818e+01) (7, 2.06938797180088940308e+01) (0, 1.69009062283779378788e+01) (1, 2.99419097769889885896e+00) (2, -1.96145524697774575884e+01) (3, 1.08984349090483291889e+02) (4, -4.96909780071390940748e+01) (5, -2.79347944559581335966e+01) (6, -4.57663323997270765631e+01) (7, 1.46741562195611052744e+01) (0, -1.34733321206381218005e+00) (1, -1.39669416046338112913e+00) (2, -2.08415686668961175343e+00) (3, 1.39601698074346014522e+01) (4, -5.99662653617671370654e+00) (5, -4.84530223111172497763e-01) (6, -4.20670818520278044783e-01) (7, -1.25572076945613397836e-01) (0, 4.84419245227208072890e+00) (1, -3.91944012862043056344e+00) (2, -5.45639997165072543339e+00) (3, -4.15097392266725861987e+00) (4, 5.28349049512807500406e+00) (5, -1.77162018076397892452e-01) (6, -5.39104803242794305973e-01) (7, 6.91061190127065483324e+00) (0, 2.68316642170591137528e+01) (1, -1.77672097227942842324e+01) (2, 9.55985678452476328459e+00) (3, -7.29421325994256530834e+00) (4, 3.70331681740250751034e+00) (5, -1.14061697353185009796e+00) (6, -4.37945463476889216281e+01) (7, 3.53080913501761202156e+01) (8, 1.19835515975315853865e+00) (9, 2.36707383729721820842e+00) (10, -4.12042730774793497517e+00) (11, -6.86129990558904179210e+00) (12, 8.02803464291737611802e+00) (13, -9.23804470077045136733e-01) 
