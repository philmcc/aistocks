FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 8 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (8, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.61282294302818218767e+00) (1, 4.79125156929185109789e+01) (2, -8.90634207883912978332e+01) (3, -1.91688927741036430064e+01) (4, -6.02642171061748186389e+01) (5, -6.07726684385113813391e+00) (6, -9.27139559028937831897e+00) (7, 4.67093215827743861723e+01) (0, 7.67684742955126520059e+00) (1, 2.31362038508500688749e+02) (2, 5.94788426529892078065e+01) (3, -4.14164897786351900777e+02) (4, -4.54826838401800443989e+01) (5, 1.71242856609471061802e+02) (6, -3.16091119593731377790e+01) (7, 1.30666511654438188827e+01) (0, -1.32454767830093260272e+03) (1, 1.13816004550938885131e+02) (2, 5.98143249635291951449e+01) (3, -7.17087492031931219572e+02) (4, 1.12048718610278751839e+03) (5, 3.12057344653672487311e+02) (6, -1.22748918979054110423e+03) (7, -7.68982168461395531267e+02) (0, 6.97783755461401028697e+02) (1, 1.54039543228438787992e+02) (2, -3.88500439880159888162e+02) (3, -2.43007227190514612403e+02) (4, -3.69117510151070575830e+02) (5, 1.40052442884508218413e+03) (6, 9.56122309119254168763e+02) (7, -9.60642751834520026932e+02) (0, -3.64188950503472767650e+02) (1, 3.62407016080631251498e+02) (2, -2.13391692480134395282e+02) (3, 4.27036997287263986323e+01) (4, -2.31492399566375269160e+02) (5, 3.99107419840822075230e+01) (6, 9.41027191931977000650e+01) (7, 2.00666296146645578347e+02) (0, -2.01005266862663560801e+01) (1, 5.38428107696910387858e+02) (2, 1.04831205265293334605e+02) (3, -3.55290896049946468338e+02) (4, -8.08640329586367272441e+01) (5, 1.00383169758527742488e+02) (6, -4.26557957862140995076e+02) (7, 2.29234130423249418129e+01) (0, -5.20993502201606133895e+02) (1, 4.24699153946836872819e+02) (2, 7.59455826247013192187e+02) (3, 1.41104812117326059706e+03) (4, 1.50000000000000000000e+03) (5, 1.50000000000000000000e+03) (6, -5.17456249102207380020e+02) (7, -1.78415501101494129443e+02) (8, 1.16370340727814003401e+00) (9, 1.56140304193806023214e+00) (10, 9.22806447640772353758e-01) (11, -2.02439048471007421170e+00) (12, -3.29909442517515083537e+00) (13, 9.25741665022633331006e-01) (14, 1.89889865079552500049e+00) (15, 1.23283496019688931788e+00) 
