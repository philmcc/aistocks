FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -2.90929073146418762374e-02) (1, -3.33152183639290611072e-02) (2, -6.09148827145231039348e-02) (3, 2.79300056834519694338e-02) (4, 8.51090312579129704140e-02) (5, -8.37911440438202370640e-02) (6, 1.12803111602412312053e-02) (7, -5.36822740537266918093e-02) (0, -8.34569915526656852522e-02) (1, -3.61646850394410565577e-02) (2, 6.99965978227048701843e-02) (3, 9.74843336825718387306e-02) (4, -9.33523100741551620230e-02) (5, -2.26759942833482316948e-02) (6, 1.74937697871591080201e-03) (7, -1.62262049127549629057e-02) (0, 1.78058326103662303841e-02) (1, 2.43418118818374204504e-02) (2, -7.33254982840343960682e-02) (3, 1.09703497733227850475e-02) (4, 9.39245976533680526632e-02) (5, -4.62745454538545722412e-02) (6, 6.80046724975805205737e-03) (7, 5.86775121640788210886e-02) (0, -2.49239290121783113596e-02) (1, -8.31346887444658205757e-03) (2, 7.27016425349062966887e-02) (3, 1.76312043164849885368e-02) (4, 5.13595888593731536576e-02) (5, -9.41345807222214170373e-02) (6, 6.60729261647600707441e-02) (7, -7.77333199453848489036e-02) (0, -2.74497975960343656987e-02) (1, -9.48419579467468937262e-02) (2, 5.01966873213155057032e-02) (3, -4.23407678282375216083e-02) (4, -7.86331005004510252832e-02) (5, -3.85230030085593824762e-02) (6, 3.97695960815190596715e-03) (7, -6.20900905630005842117e-02) (0, 2.53123134421156803509e-02) (1, -2.60264440592593501722e-02) (2, -6.46057582774126060166e-02) (3, 3.19600048580766377126e-02) (4, 5.12975632406407963670e-02) (5, 3.71436201914194172313e-02) (6, -8.42662015447944445778e-02) (7, -3.08966056391090926336e-02) (0, -3.85145694168592817030e-02) (1, -5.75916983387127143224e-02) (2, 8.00737457174620637090e-02) (3, -4.45899732536073414857e-02) (4, -3.86624220931890832942e-03) (5, -1.31257885228959966795e-02) (6, -8.59124625796446328430e-02) (7, 7.12098302686189066346e-02) (0, 7.85607440927735406477e-02) (1, 8.67891814453777693528e-02) (2, -1.11589669050122311522e-02) (3, 2.99203314620305749205e-02) (4, 9.26546022132724578224e-02) (5, -4.50860422303682797929e-02) (6, 5.21870130998941111899e-02) (7, -3.47951968728780203222e-02) (0, -3.99279986869990541343e-02) (1, 2.38369893109349750837e-03) (2, 2.28640368821328363036e-02) (3, -1.85610976973339530938e-02) (4, 6.38606974126502413558e-02) (5, -7.31590049998313840529e-02) (6, 1.93488133229138409286e-02) (7, -1.08269906353502046170e-02) (0, 8.14552524157650947867e-04) (1, 5.47430565356173473579e-02) (2, -7.88669872673896932280e-02) (3, -4.78878857253176720699e-02) (4, -8.11332476307935479554e-03) (5, -6.31331872289357526329e-02) (6, 2.12155101256893546813e-02) (7, 5.33721074033097417355e-02) (8, -2.07248840775323475705e-02) (9, 1.28925435303530594444e-03) (10, -9.12178673404137191350e-02) (11, 7.54088752032648634849e-02) (12, 8.81634673202554286497e-02) (13, -7.71303283368099806827e-02) (14, 4.66187039817676437958e-02) (15, 6.67242099229128499127e-02) (16, -9.03411483815483307147e-02) (17, -6.45402644133607067412e-02) (18, -3.35546001204043570221e-03) 
