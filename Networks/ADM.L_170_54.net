FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 6 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (6, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -4.32815628037151123308e-01) (1, -4.63256687851479220797e+00) (2, -2.42584389726376137286e-01) (3, 2.02691336072032868287e+00) (4, 7.23276130225785740535e-01) (5, -1.62595152128966491389e+00) (6, 4.58676718231835778283e+00) (7, 3.41747251826668518859e+00) (0, 1.27324463440907516087e+03) (1, -8.18348245604595831537e+02) (2, 4.37180721678869701918e+02) (3, -1.50000000000000000000e+03) (4, -4.12810671695563655703e+02) (5, 1.28791806811027322510e+03) (6, 1.05805603361105752924e+02) (7, 2.62524995551405652350e+02) (0, 1.69440928731410500596e+00) (1, 2.88224663515479733178e+01) (2, 3.13657216559164631065e+01) (3, -9.57218697022856304102e+01) (4, 1.47733779078397038376e+01) (5, 2.33214764532319627222e+01) (6, -2.52051386925844376208e+01) (7, 2.24480263773874604283e+01) (0, 3.69416141256549046545e+00) (1, 1.71706280402671431773e+01) (2, 7.26992122307029919881e+00) (3, 4.20460675968968633498e+00) (4, 1.04135238792268989272e+01) (5, -2.89205614713883418787e+00) (6, -2.32514784726273333604e+01) (7, 2.56898130882918629414e-01) (0, 2.92603445868607936475e+00) (1, -2.06158503526605905165e+01) (2, -1.04154509220542934145e+02) (3, 1.22915597110370509171e+01) (4, -8.02611999118831107580e+01) (5, 5.75167196650375256439e+00) (6, 4.64432694514251949158e+01) (7, 4.31801333591755067687e+01) (8, -4.20866674673677199081e+00) (9, -1.90790338984659491040e+00) (10, 2.09573924139908340791e+00) (11, 4.00679884152940246622e+00) (12, 1.67985919114611448322e+00) (13, -1.91882895070367681667e-01) 
