FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 11 3 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -3.74579360702326225052e-03) (1, 7.07619401926091184940e-02) (2, 4.06018213831159588412e-02) (3, -5.86259480919830086587e-02) (4, 6.68499235711980249741e-02) (5, -3.72232735115777396695e-02) (6, -9.75302239511940255445e-03) (7, 5.99027467504552879607e-02) (0, -2.27862344724868853829e-02) (1, -1.93987323067893388528e-02) (2, 4.13047926136075257975e-02) (3, 5.22491618394726672903e-02) (4, 4.30826952581509145523e-02) (5, -7.20299864676592627166e-02) (6, 8.30421283176858288089e-03) (7, -6.23165294500668126565e-02) (0, -3.11213829535451902419e-04) (1, -5.57678083817334108163e-02) (2, -5.77128600547143696953e-02) (3, 6.77783091407762189329e-02) (4, -4.84597780512240131068e-02) (5, 8.65393690621758665538e-02) (6, -3.79751443002394398207e-02) (7, -3.88603006692275418366e-02) (0, 9.90733339313169258933e-02) (1, -9.51598897786421760170e-02) (2, -9.64936210251393922110e-02) (3, -2.88427437296918526277e-02) (4, 3.87417703120674153383e-02) (5, 7.66292487513906828145e-02) (6, -3.40823145073997080257e-03) (7, -6.50040247850719732359e-02) (0, 4.73911874538836888626e-02) (1, -6.28064115577401382851e-02) (2, -2.36299713869388555709e-02) (3, 1.42411095349655944520e-02) (4, -2.96835792017516308938e-05) (5, 6.66170077080578681983e-02) (6, -2.58561452046952369721e-02) (7, 7.71840834384274754321e-02) (0, -5.27817259957153381289e-02) (1, -8.45513540812038305594e-02) (2, 2.94332438809162891258e-02) (3, 9.03009707525516958082e-02) (4, -5.65813390587469669524e-02) (5, -6.22625447774312473781e-02) (6, -7.20155600944989704448e-02) (7, 4.31074486018337005300e-02) (0, -1.80303516690485388096e-02) (1, -2.97284186590972276942e-02) (2, 1.08857562524938000781e-02) (3, 3.35098717698435674683e-02) (4, -4.31890509939052216759e-02) (5, 7.29106134423704865810e-02) (6, 9.46495726838643969270e-02) (7, -4.41157185527044221063e-02) (0, 7.77507251538444160710e-02) (1, 9.81559531488411379785e-02) (2, 2.70415392077198446508e-02) (3, 1.64924939757957120245e-02) (4, 7.47852004101157152860e-02) (5, -7.63666937331362455366e-02) (6, 5.14884706808398651123e-02) (7, 2.21763863738832778250e-02) (0, -3.91731038007602574980e-02) (1, -7.21415021030828440551e-02) (2, -6.35825055812672401689e-02) (3, 6.07972141101541102559e-02) (4, 9.44755070950911296501e-02) (5, 1.05613507972858941542e-02) (6, 3.79812960584654732421e-02) (7, -5.83062203907403278635e-02) (0, 2.60099982061981829795e-02) (1, -3.25854615507343570169e-02) (2, -6.80052510351724925908e-02) (3, 6.94286607306995873223e-02) (4, 5.15199516195051498979e-03) (5, -4.00208096395553505897e-02) (6, 1.25361078424171754064e-02) (7, 8.71216449830180955649e-02) (8, 3.02507731914635411008e-02) (9, -7.65781373020728850509e-02) (10, 2.06315153558778024978e-02) (11, 8.70617236876744388097e-02) (12, 9.63324776304137209149e-02) (13, 1.52810865496260869789e-02) (14, -5.70539963551461026814e-02) (15, 7.40832013872742695115e-02) (16, 1.34370382083511055726e-02) (17, 6.99875443426898613541e-02) (18, -9.42430612704613784869e-03) (8, -1.17777628716493054650e-02) (9, 9.36208520996697352023e-02) (10, -5.79358368431901332718e-02) (11, -8.96013779878821470248e-02) (12, -4.55522531912066416804e-02) (13, -3.00773374561568648811e-02) (14, -5.31838820790332747479e-02) (15, -8.47550405711686438703e-02) (16, -3.56018318511818546157e-02) (17, 5.73774702083687387910e-02) (18, 5.32262570705452006670e-02) 
