FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 10 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (10, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -6.47301969450741476741e-02) (1, -4.02038150869878305183e-02) (2, 4.41444418451851783303e-02) (3, -4.71102721558551706948e-02) (4, 4.76545460172674850696e-02) (5, -1.67422114572258473597e-02) (6, 9.68918289526079223872e-02) (7, -1.57723909731565525849e-02) (0, -4.34936448893494045920e-02) (1, 2.36165099571125150502e-02) (2, -6.36971692733044603241e-02) (3, 8.67105698095346766330e-02) (4, 7.86986255638096321441e-02) (5, -3.04633860359100339110e-02) (6, 1.93048489362723924012e-02) (7, -1.69993698336090084955e-02) (0, -2.05155033881292206410e-02) (1, 6.38274030957728943925e-02) (2, -6.07771105807588946091e-02) (3, -8.72577004098549352529e-02) (4, 5.44211772929307294189e-02) (5, -6.34951646588022000639e-02) (6, -4.70158692004661038499e-02) (7, -1.46963229730177336041e-02) (0, -5.59130729125897896203e-03) (1, -3.70379701187904114557e-03) (2, 6.20071924218116923844e-02) (3, -3.33756234813574401143e-02) (4, 9.96136939095510903774e-02) (5, -8.69661951358049190031e-02) (6, -6.65821648811126265421e-02) (7, -6.51165045256391628037e-02) (0, -2.71700087326766301365e-02) (1, 7.75622785473209230833e-02) (2, -1.22267751913782210527e-02) (3, -7.95154642055252713906e-02) (4, -3.91799344000210436612e-02) (5, -1.53349477288864180502e-02) (6, 4.71214640456656119749e-03) (7, 1.73264222938779299810e-02) (0, -9.17184391687577704744e-02) (1, 4.10149786213782133193e-02) (2, 4.03699061329648722918e-03) (3, 8.69801878851679949323e-02) (4, -8.94484089046479469154e-02) (5, -7.66581618474149739662e-02) (6, -3.00191834385571329480e-02) (7, -9.96391080266104123275e-03) (0, 8.71692427384740398111e-02) (1, 9.20370756393235067705e-03) (2, 2.77839037073240174847e-03) (3, 4.15904186344209017556e-02) (4, 4.57085443952462630590e-02) (5, 5.57625226603824172833e-02) (6, -7.31059058287129581721e-02) (7, -5.98827642929965764385e-02) (0, -4.79412758416127432470e-02) (1, 8.89012880832148605359e-02) (2, 6.74161371576210283196e-03) (3, -4.83275834221777722544e-02) (4, -9.80649085427061778519e-02) (5, 4.01594504178978475850e-02) (6, -1.34440863645685637628e-02) (7, -2.52349157852666886037e-02) (0, 1.77217274751026512836e-02) (1, 7.43291399341693415082e-02) (2, -4.75037840754357482131e-03) (3, 7.85417945651977200683e-02) (4, -4.10058091917009440164e-02) (5, 9.99617694871391126998e-02) (6, -4.13178463104046239662e-03) (7, -3.27242468703425881671e-02) (8, 4.09767466184011996955e-02) (9, 9.99052075655044030666e-02) (10, -4.57440604752907126196e-02) (11, 5.15283392970016379531e-02) (12, -7.67529557720267041621e-02) (13, 2.42367575762682738172e-02) (14, -5.84355729957755226645e-02) (15, -8.95837145236687837357e-02) (16, -6.65595363499154879516e-02) (17, 4.43428188650729984688e-02) 
