FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 8 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (8, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.57518970986040258708e+01) (1, 1.42607733442913509059e+00) (2, -4.49347719216482701654e+01) (3, 6.12615827824123897472e+01) (4, 2.11676075961797778291e+01) (5, 2.39421782086296808245e+01) (6, 1.66108604974173843516e+01) (7, -2.14999186801215635967e+01) (0, -1.73995319433972035483e+01) (1, 1.83477611616017846075e+01) (2, 1.38555071563517895328e+00) (3, 5.98612313779816584969e+00) (4, 7.31625990208630749834e+00) (5, -5.36218830631482301641e+00) (6, -1.92200443068577762062e+01) (7, 1.71103130587299787635e+01) (0, 1.50000000000000000000e+03) (1, 2.89734022492051224162e+01) (2, 1.50000000000000000000e+03) (3, 1.50000000000000000000e+03) (4, 1.08090674114960393126e+03) (5, 1.50000000000000000000e+03) (6, 1.50000000000000000000e+03) (7, -1.49438746103683661204e+03) (0, 5.74635143467605416845e+02) (1, 7.16215291839863539280e+02) (2, -2.65318347416854365406e+01) (3, 1.78455085413773360870e+02) (4, -1.07502292009714437881e+03) (5, 1.95945009970910462016e+02) (6, 4.71403326231250616729e+02) (7, 2.51369538480216533571e+02) (0, 1.16350304228291445696e+01) (1, -7.18421398949086942309e+00) (2, -1.16198955742346998754e+00) (3, -2.19587776525403466366e+00) (4, 2.52085632183194219280e+00) (5, -2.13811654178710230934e-01) (6, -2.52440950509421346837e+00) (7, -2.14467703215971594588e+00) (0, -3.33730969219223823075e+02) (1, 3.19192870207621865575e+02) (2, -1.15351008728643165568e+02) (3, 5.93172109886678669000e+01) (4, 1.52185597006094841532e+01) (5, -3.65007777708806946393e+00) (6, 2.45540687923584783903e+02) (7, -1.01114101143386658777e+01) (0, 1.50000000000000000000e+03) (1, 1.50000000000000000000e+03) (2, -6.11844781319656931373e+02) (3, 5.03145299361576292085e+02) (4, -6.75013426887055061343e+02) (5, -1.50000000000000000000e+03) (6, -1.40438534203425069791e+03) (7, 1.50000000000000000000e+03) (8, -1.95304356900749209736e+00) (9, 8.17804618232568891756e+00) (10, 1.43531338464228475083e+00) (11, -9.96565002580959591327e-01) (12, 3.06406650882722830787e+00) (13, -1.35171378157280175358e+00) (14, -1.05220198088281513549e+00) (15, -4.95541815537768748356e+00) 
