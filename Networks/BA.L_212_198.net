FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 8 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (8, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 3.05732772750249672811e+01) (1, -1.83726054124721009941e+01) (2, 1.37745864172955005955e+01) (3, 2.23714509334497524407e+00) (4, -1.40416083414211474434e+01) (5, 1.17640031617157934107e+01) (6, -3.38413752238001785955e+00) (7, 2.54529031625963853003e+00) (0, 1.73028411974381057803e+01) (1, -1.72105589299196424236e+01) (2, 5.34248936508871530293e+00) (3, 7.16913211893849489087e+00) (4, -8.06364321588755039727e+00) (5, -8.96549408644786161027e-01) (6, -8.26426728234987972854e-01) (7, -4.52100092191032398858e+00) (0, -3.57415824656669158799e+01) (1, -5.12934536895129937761e+01) (2, 4.25121529136512847913e+01) (3, 2.51255671889267233610e+01) (4, 9.01292202507070072670e+01) (5, -2.54306255340064311099e+01) (6, 1.98772964165031424955e+01) (7, -1.83671911838882202517e+01) (0, 1.31883356689430062403e+01) (1, -4.01415299603007103002e+00) (2, -2.68449756265073125050e+00) (3, 4.89051632105623246272e-01) (4, -3.97311148662957647204e+00) (5, -1.65845867172764660502e+00) (6, -1.33561568087882664990e+00) (7, 1.24499324155329427199e+00) (0, -3.24859228259330336641e+02) (1, -3.24316064129743111266e+01) (2, 2.08862612290020024375e+02) (3, 1.23463569279398356571e+02) (4, -7.78375224110913563891e+01) (5, 1.70878826541114250404e+02) (6, 7.59381738536788617466e+01) (7, -1.05676295804727487848e+02) (0, 4.69567983266850390578e+01) (1, -1.50139260090039556417e+02) (2, 7.61256801997177205976e+01) (3, -2.64752587076650513609e+02) (4, -5.58108619532875209757e+01) (5, -1.02166257610761974206e+02) (6, 2.11600333014479787153e+02) (7, 6.40867955758968150803e+01) (0, 1.21186419679098325020e+01) (1, -1.28926664939734028081e+01) (2, 1.73585832579541160525e-01) (3, 4.11513980404054791062e-01) (4, -1.48896428985632045539e+01) (5, -2.96501162491014147449e+00) (6, 8.20593103767234843815e+00) (7, 3.01963152195373729647e-01) (8, 6.16693921505083331169e+00) (9, 6.59174659633359194544e+00) (10, -2.08890703013377709851e+00) (11, -3.51237121773328819785e+00) (12, -1.71205428647284696453e+00) (13, 1.91786481327981039868e+00) (14, -8.22863728815149642060e+00) (15, -1.78920917195592443782e+00) 
